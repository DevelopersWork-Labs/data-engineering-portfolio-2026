# Databricks notebook source
# MAGIC %md
# MAGIC ## Step 1Ô∏è‚É£ ‚Äî Install & Import

# COMMAND ----------

# MAGIC %pip install tiktoken

# COMMAND ----------

import tiktoken

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 2Ô∏è‚É£ ‚Äî Load a tokenizer

# COMMAND ----------

enc = tiktoken.get_encoding("cl100k_base")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 3Ô∏è‚É£ ‚Äî Tokenize simple text vs number

# COMMAND ----------

text_word = "Data"
text_number = "794625452451"

print("Word tokens:", enc.encode(text_word))
print("Number tokens:", enc.encode(text_number))

print("Word token count:", len(enc.encode(text_word)))
print("Number token count:", len(enc.encode(text_number)))

# COMMAND ----------

# MAGIC %md
# MAGIC **Output:**
# MAGIC ```
# MAGIC Word tokens: [1061]
# MAGIC Number tokens: [25926, 15894, 21098, 20360]
# MAGIC Word token count: 1
# MAGIC Number token count: 4
# MAGIC ```

# COMMAND ----------

enc.encode("2024-09-18T12:45:33Z")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üß† WHY THIS HAPPENS (This is the core insight)
# MAGIC **LLM tokenizers are:**
# MAGIC - Optimized for natural language
# MAGIC - Trained on common word patterns
# MAGIC - BAD at long, random-looking numbers
# MAGIC
# MAGIC **So:**
# MAGIC - `Data` ‚Üí seen millions of times ‚Üí single token
# MAGIC - `794625452451` ‚Üí rare pattern ‚Üí split into chunks